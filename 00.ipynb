{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5a48ba-e536-4d94-826a-de20eed85f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import *\n",
    "import torch,torchvision\n",
    "from tqdm import tqdm\n",
    "device = 'cuda'\n",
    "PROJECT_NAME = 'Intel-Image-Clf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72391b08-481f-4791-80f1-35bbc51e4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = {}\n",
    "    idx = -1\n",
    "    for folder in tqdm(os.listdir('./data/')):\n",
    "        idx += 1\n",
    "        labels[folder] = idx\n",
    "        for file in os.listdir(f'./data/{folder}/'):\n",
    "            file = f'./data/{folder}/{file}'\n",
    "            img = cv2.imread(file)\n",
    "            img = cv2.resize(img,(112,112))\n",
    "            X.append(img/255.0)\n",
    "            y.append(labels[folder])\n",
    "    return X,y,idx,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52fe36d-c567-4f21-86b7-f827f6eafad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:11<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "X,y,idx,labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4da8164-4706-4f15-b24d-3367be780323",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = {}\n",
    "for l_key,l_val in zip(labels.keys(),labels.values()):\n",
    "    new_labels[l_val] = l_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679ac0f4-3392-4324-b166-cd6218251209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e837861-3b32-484f-80c8-405e62da7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb6c5c9-14a8-492c-a03d-4f144b802342",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,test_size=0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "481debf1-1e2a-4c0a-b1b2-83431080e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.array(X_train))\n",
    "X_test = torch.from_numpy(np.array(X_test))\n",
    "y_train = torch.from_numpy(np.array(y_train))\n",
    "y_test = torch.from_numpy(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fb1ee4-6552-4ae8-8518-c23e775d3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(preds,y,criterion):\n",
    "    loss = criterion(preds,y)\n",
    "    return loss.item()\n",
    "def get_accuracy(preds,y):\n",
    "    total = -1\n",
    "    correct = -1\n",
    "    for pred,y_batch in zip(preds,y):\n",
    "        pred = torch.argmax(pred)\n",
    "        pred = round(float(pred))\n",
    "        y_batch = round(float(y_batch))\n",
    "        if pred == y_batch:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return round(correct/total,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353f25d7-904b-4252-a53a-91da5941626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_test_data(model):\n",
    "    preds = []\n",
    "    for file in os.listdir('./test_data/'):\n",
    "        img = cv2.imread(f'./test_data/{file}')\n",
    "        img = cv2.resize(img,(112,112))\n",
    "        model.eval()\n",
    "        pred = model(torch.from_numpy(img).to(device).float().view(-1,3,112,112))\n",
    "        plt.figure(figsize=(12,7))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'{new_labels[int(torch.argmax(pred))]}')\n",
    "        plt.savefig(f'./preds/{file}')\n",
    "        plt.close()\n",
    "        preds.append(torch.argmax(pred))\n",
    "    model.train()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f424ab7-ec4a-4ef0-ad1c-c0d66b96230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool2d = MaxPool2d((2,2),(2,2))\n",
    "        self.activation = ReLU()\n",
    "        self.conv1 = Conv2d(3,6,(3,3))\n",
    "        self.conv1batchnorm = BatchNorm2d(6)\n",
    "        self.conv2 = Conv2d(6,8,(3,3))\n",
    "        self.conv2batchnorm = BatchNorm2d(8)\n",
    "        self.conv3 = Conv2d(8,10,(5,5))\n",
    "        self.conv3batchnorm = BatchNorm2d(10)\n",
    "        self.conv4 = Conv2d(10,12,(5,5))\n",
    "        self.conv4batchnorm = BatchNorm2d(12)\n",
    "        self.linear1 = Linear(12*3*3,32)\n",
    "        self.linear1batchnorm = BatchNorm1d(32)\n",
    "        self.linear2 = Linear(32,64)\n",
    "        self.linear2batchnorm = BatchNorm1d(64)\n",
    "        self.linear3 = Linear(64,128)\n",
    "        self.linear3batchnorm = BatchNorm1d(128)\n",
    "        self.linear4 = Linear(128,256)\n",
    "        self.linear4batchnorm = BatchNorm1d(256)\n",
    "        self.linear5 = Linear(256,128)\n",
    "        self.linear5batchnorm = BatchNorm1d(128)\n",
    "        self.output = Linear(128,6)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        preds = X\n",
    "        preds = self.max_pool2d(self.activation(self.conv1batchnorm(self.conv1(preds))))\n",
    "        preds = self.max_pool2d(self.activation(self.conv2batchnorm(self.conv2(preds))))\n",
    "        preds = self.max_pool2d(self.activation(self.conv3batchnorm(self.conv3(preds))))\n",
    "        preds = self.max_pool2d(self.activation(self.conv4batchnorm(self.conv4(preds))))\n",
    "        preds = preds.view(-1,12*3*3)\n",
    "        preds = self.activation(self.linear1batchnorm(self.linear1(preds)))\n",
    "        preds = self.activation(self.linear2batchnorm(self.linear2(preds)))\n",
    "        preds = self.activation(self.linear3batchnorm(self.linear3(preds)))\n",
    "        preds = self.activation(self.linear4batchnorm(self.linear4(preds)))\n",
    "        preds = self.activation(self.linear5batchnorm(self.linear5(preds)))\n",
    "        preds = self.output(preds)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2da0c6e4-788b-4f8f-a391-f7987317e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "536a7521-a299-4bf4-a4f9-e5eab333fbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (max_pool2d): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (activation): ReLU()\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv1batchnorm): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2batchnorm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3batchnorm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(10, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv4batchnorm): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=108, out_features=32, bias=True)\n",
       "  (linear1batchnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (linear2batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear3): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (linear3batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear4): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (linear4batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (linear5batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (output): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e5f272-55aa-4d10-8483-7b9d1ed2fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1141367d-b358-4665-a294-6eb1943e7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d64b048e-7761-40f4-9bb7-8b4f80111833",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bec8fe3b-0f8f-4c6d-8644-f50f271ba7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee6182f5-b6d0-4f6c-b4a5-d18c5143d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=  32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b54adcd-49ef-4156-8787-8fe8f38de576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "345bfeac-fb99-454a-84d5-6d8f46189d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glacier': 0,\n",
       " 'street': 1,\n",
       " 'forest': 2,\n",
       " 'sea': 3,\n",
       " 'buildings': 4,\n",
       " 'mountain': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c23d99-012d-468b-a4c0-a99532fb867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranuga-d\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">baseline</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ranuga-d/Intel-Image-Clf\" target=\"_blank\">https://wandb.ai/ranuga-d/Intel-Image-Clf</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ranuga-d/Intel-Image-Clf/runs/2urc8yal\" target=\"_blank\">https://wandb.ai/ranuga-d/Intel-Image-Clf/runs/2urc8yal</a><br/>\n",
       "                Run data is saved locally in <code>/home/indika/Programming/Projects/Python/Artifical-Intelligence/PyTorch/CNN/Intel-Image-Clf/wandb/run-20210901_143359-2urc8yal</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [05:04<04:36,  6.15s/it]"
     ]
    }
   ],
   "source": [
    "wandb.init(project=PROJECT_NAME,name='baseline')\n",
    "wandb.watch(model)\n",
    "for _ in tqdm(range(epochs)):\n",
    "    for i in range(0,len(X_train),batch_size):\n",
    "        X_batch = X_train[i:i+batch_size].to(device).float().view(-1,3,112,112)\n",
    "        y_batch = y_train[i:i+batch_size].to(device)\n",
    "        preds = model(X_batch)\n",
    "        preds = preds.to(device)\n",
    "        loss = criterion(preds,y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    wandb.log({'Loss':loss.item()})\n",
    "    wandb.log({'Val Loss':get_loss(model(X_test.to(device).float().view(-1,3,112,112)),y_test.to(device),criterion)})\n",
    "    wandb.log({'Accuracy':get_accuracy(preds,y_batch)})\n",
    "    wandb.log({'Val Accuracy':get_accuracy(model(X_test.to(device).float().view(-1,3,112,112)),y_test.to(device))})\n",
    "    pred_test_data(model)\n",
    "    for file in os.listdir('./preds/'):\n",
    "        wandb.log({f'Img/{file}':wandb.Image(cv2.imread(f'./preds/{file}'))})\n",
    "    wandb.watch(model)\n",
    "wandb.watch(model)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27a717-e70e-46df-bb71-97b9acddc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [8,16,32,64,128,256,512,1024,2048]\n",
    "for batch_size in batch_sizes\n",
    "    wandb.init(project=PROJECT_NAME,name=f'batch_size-{batch_size}')\n",
    "    wandb.watch(model)\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for i in range(0,len(X_train),batch_size):\n",
    "            X_batch = X_train[i:i+batch_size].to(device).float().view(-1,3,112,112)\n",
    "            y_batch = y_train[i:i+batch_size].to(device)\n",
    "            preds = model(X_batch)\n",
    "            preds = preds.to(device)\n",
    "            loss = criterion(preds,y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        wandb.log({'Loss':loss.item()})\n",
    "        wandb.log({'Val Loss':get_loss(model(X_test.to(device).float().view(-1,3,112,112)),y_test.to(device),criterion)})\n",
    "        wandb.log({'Accuracy':get_accuracy(preds,y_batch)})\n",
    "        wandb.log({'Val Accuracy':get_accuracy(model(X_test.to(device).float().view(-1,3,112,112)),y_test.to(device))})\n",
    "        pred_test_data(model)\n",
    "        for file in os.listdir('./preds/'):\n",
    "            wandb.log({f'Img/{file}':wandb.Image(cv2.imread(f'./preds/{file}'))})\n",
    "        wandb.watch(model)\n",
    "    wandb.watch(model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633446e-9be3-420c-afdd-5c04339c88fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python373jvsc74a57bd0210f9608a45c0278a93c9e0b10db32a427986ab48cfc0d20c139811eb78c4bbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
